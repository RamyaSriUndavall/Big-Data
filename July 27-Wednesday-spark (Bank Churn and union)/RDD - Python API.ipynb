{"cells":[{"cell_type":"code","source":["from pyspark import SparkConf,SparkContext\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af98acb4-594d-4cd9-ae83-3fd8ced7a5fe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["conf=SparkConf().setAppName('RDD')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95cb3cea-c47d-40d7-b7ea-5c254a0c639c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc=SparkContext(conf=conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fd302fe-3a8c-4a42-8741-42bbde32794c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-3102105807951545>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    357\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    359\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-3102105807951545>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    357\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    359\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "]}}],"execution_count":0},{"cell_type":"code","source":["data=[\"spark is a bigdata solution\",\"hadoop and spark both we can use to haldle bigdata\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2af07f4a-79fb-48ca-a708-56ee376c66f8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd=sc.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8038cc1f-964c-4db1-a9ca-f308df369f3a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed976b30-33c7-4b5c-b571-e9cb8edc9712"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: ParallelCollectionRDD[29] at readRDDFromInputStream at PythonRDD.scala:413","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: ParallelCollectionRDD[29] at readRDDFromInputStream at PythonRDD.scala:413"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa8d4bbe-d80e-46bc-8f4d-d69113f7dc95"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: ['spark is a bigdata solution',\n 'hadoop and spark both we can use to haldle bigdata']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: ['spark is a bigdata solution',\n 'hadoop and spark both we can use to haldle bigdata']"]}}],"execution_count":0},{"cell_type":"code","source":["def upper(s):\n  return s.upper()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8c3d132-aa70-490c-81f4-2e1e168b8970"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stringrdd=rdd.map(upper)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5321d6f4-67f0-452f-bde3-53422bed77a3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stringrdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3036387b-5283-4665-bf41-68cdb4a66d75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: PythonRDD[30] at RDD at PythonRDD.scala:58","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: PythonRDD[30] at RDD at PythonRDD.scala:58"]}}],"execution_count":0},{"cell_type":"code","source":["stringrdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95f03ba7-f26f-45e8-ad56-71bd566a0edc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: ['SPARK IS A BIGDATA SOLUTION',\n 'HADOOP AND SPARK BOTH WE CAN USE TO HALDLE BIGDATA']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: ['SPARK IS A BIGDATA SOLUTION',\n 'HADOOP AND SPARK BOTH WE CAN USE TO HALDLE BIGDATA']"]}}],"execution_count":0},{"cell_type":"code","source":["data=[78,56,89,45,26,78,59,56,47]\nrdd=sc.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d78e7760-87ff-4216-9c21-13d3e901cdbe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd2=rdd.filter(lambda z:z%2==0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7974933-0dbf-4c8c-91c6-9a405fa75a2c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rdd2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c78e0786-e942-44a2-b9aa-63733c27865a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: PythonRDD[32] at RDD at PythonRDD.scala:58","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: PythonRDD[32] at RDD at PythonRDD.scala:58"]}}],"execution_count":0},{"cell_type":"code","source":["rdd2.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4683217-b6ba-4615-9846-b4be92736a3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[17]: [78, 56, 26, 78, 56]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: [78, 56, 26, 78, 56]"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24a7845e-9b10-4f87-acdc-f1df3e9f3fea"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"RDD - Python API","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3102105807951542}},"nbformat":4,"nbformat_minor":0}
