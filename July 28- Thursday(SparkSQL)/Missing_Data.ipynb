{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6da37d90-f9be-4a1d-a1dd-83255dcca6dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Missing Data\n",
    "\n",
    "Often data sources are incomplete, which means you will have missing data, you have 3 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "* Just keep the missing data points.\n",
    "* Drop them missing data points (including the entire row)\n",
    "* Fill them in with some other value.\n",
    "\n",
    "Let's cover examples of each of these methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "340a3b8e-b18d-43bc-81fe-f62f663337fb",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "source": [
    "## Keeping the missing data\n",
    "A few machine learning algorithms can easily deal with missing data, let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3257bb5-1762-4f9c-bea0-b3aeab4cb102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"missingdata\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ce53f98-f69e-462a-b5dc-ecfd5aa4942e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"ContainsNull.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "729ef834-2d58-40fd-9425-a8466cc01b01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John| null|\n",
       "|emp2| null| null|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp2| null| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b183fdc-c1d7-4307-8dcf-f07ce9c14041",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice how the data remains as a null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be2e98da-c360-47ab-8935-af47f76d3c57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.\n",
    "    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.\n",
    "    \n",
    "    * param thresh: int, default None\n",
    "    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.\n",
    "        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45d63fc0-f96c-400c-86fe-159739d73b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop any row that contains missing data\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d640c8ba-b213-42ec-8236-4f0a485b66bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John| null|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Has to have at least 2 NON-null values\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abcc48ab-eafd-434a-aa3f-277aa5b08c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.drop(subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e75aa7de-7042-446a-9431-b1691f6e1cbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7eade96f-476c-4a3e-8db4-4a2ca1bcf366",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John| null|\n",
       "|emp2| null| null|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John| null|\n|emp2| null| null|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a280f678-2b8c-41fa-b064-27da5c7367bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f448ee16-d329-4f85-898c-4e9207fb69fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+---------+-----+\n",
       "|  Id|     Name|Sales|\n",
       "+----+---------+-----+\n",
       "|emp1|     John| null|\n",
       "|emp2|NEW VALUE| null|\n",
       "|emp3|NEW VALUE|345.0|\n",
       "|emp4|    Cindy|456.0|\n",
       "+----+---------+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+---------+-----+\n|  Id|     Name|Sales|\n+----+---------+-----+\n|emp1|     John| null|\n|emp2|NEW VALUE| null|\n|emp3|NEW VALUE|345.0|\n|emp4|    Cindy|456.0|\n+----+---------+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill('NEW VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e765aa8c-eca3-473a-aaac-082762cfe865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John|  0.0|\n",
       "|emp2| null|  0.0|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John|  0.0|\n|emp2| null|  0.0|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba3da00a-e836-4944-9109-4c03e7c02ecb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "439345a0-61b6-47f1-a0b9-e584ab9e889c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-------+-----+\n",
       "|  Id|   Name|Sales|\n",
       "+----+-------+-----+\n",
       "|emp1|   John| null|\n",
       "|emp2|No Name| null|\n",
       "|emp3|No Name|345.0|\n",
       "|emp4|  Cindy|456.0|\n",
       "+----+-------+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-------+-----+\n|  Id|   Name|Sales|\n+----+-------+-----+\n|emp1|   John| null|\n|emp2|No Name| null|\n|emp3|No Name|345.0|\n|emp4|  Cindy|456.0|\n+----+-------+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill('No Name',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccf6109c-3c1b-4631-b78b-2f4ca3c69100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Help on method fill in module pyspark.sql.dataframe:\n",
       "\n",
       "fill(value, subset=None) method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n",
       "    Replace null values, alias for ``na.fill()``.\n",
       "    :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
       "    \n",
       "    :param value: int, long, float, string, bool or dict.\n",
       "        Value to replace null values with.\n",
       "        If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
       "        from column name (string) to replacement value. The replacement value must be\n",
       "        an int, long, float, boolean, or string.\n",
       "    :param subset: optional list of column names to consider.\n",
       "        Columns specified in subset that do not have matching data type are ignored.\n",
       "        For example, if `value` is a string, and subset contains a non-string column,\n",
       "        then the non-string column is simply ignored.\n",
       "    \n",
       "    >>> df4.na.fill(50).show()\n",
       "    +---+------+-----+\n",
       "    |age|height| name|\n",
       "    +---+------+-----+\n",
       "    | 10|    80|Alice|\n",
       "    |  5|    50|  Bob|\n",
       "    | 50|    50|  Tom|\n",
       "    | 50|    50| null|\n",
       "    +---+------+-----+\n",
       "    \n",
       "    >>> df5.na.fill(False).show()\n",
       "    +----+-------+-----+\n",
       "    | age|   name|  spy|\n",
       "    +----+-------+-----+\n",
       "    |  10|  Alice|false|\n",
       "    |   5|    Bob|false|\n",
       "    |null|Mallory| true|\n",
       "    +----+-------+-----+\n",
       "    \n",
       "    >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
       "    +---+------+-------+\n",
       "    |age|height|   name|\n",
       "    +---+------+-------+\n",
       "    | 10|    80|  Alice|\n",
       "    |  5|  null|    Bob|\n",
       "    | 50|  null|    Tom|\n",
       "    | 50|  null|unknown|\n",
       "    +---+------+-------+\n",
       "    \n",
       "    .. versionadded:: 1.3.1\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Help on method fill in module pyspark.sql.dataframe:\n\nfill(value, subset=None) method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n    Replace null values, alias for ``na.fill()``.\n    :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n    \n    :param value: int, long, float, string, bool or dict.\n        Value to replace null values with.\n        If the value is a dict, then `subset` is ignored and `value` must be a mapping\n        from column name (string) to replacement value. The replacement value must be\n        an int, long, float, boolean, or string.\n    :param subset: optional list of column names to consider.\n        Columns specified in subset that do not have matching data type are ignored.\n        For example, if `value` is a string, and subset contains a non-string column,\n        then the non-string column is simply ignored.\n    \n    >>> df4.na.fill(50).show()\n    +---+------+-----+\n    |age|height| name|\n    +---+------+-----+\n    | 10|    80|Alice|\n    |  5|    50|  Bob|\n    | 50|    50|  Tom|\n    | 50|    50| null|\n    +---+------+-----+\n    \n    >>> df5.na.fill(False).show()\n    +----+-------+-----+\n    | age|   name|  spy|\n    +----+-------+-----+\n    |  10|  Alice|false|\n    |   5|    Bob|false|\n    |null|Mallory| true|\n    +----+-------+-----+\n    \n    >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n    +---+------+-------+\n    |age|height|   name|\n    +---+------+-------+\n    | 10|    80|  Alice|\n    |  5|  null|    Bob|\n    | 50|  null|    Tom|\n    | 50|  null|unknown|\n    +---+------+-------+\n    \n    .. versionadded:: 1.3.1\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(df.na.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aac9cc69-edef-446f-8604-feba0bf52996",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-------+-----+\n",
       "|  Id|   Name|Sales|\n",
       "+----+-------+-----+\n",
       "|emp1|   John|  0.0|\n",
       "|emp2|No Name|  0.0|\n",
       "|emp3|No Name|345.0|\n",
       "|emp4|  Cindy|456.0|\n",
       "+----+-------+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-------+-----+\n|  Id|   Name|Sales|\n+----+-------+-----+\n|emp1|   John|  0.0|\n|emp2|No Name|  0.0|\n|emp3|No Name|345.0|\n|emp4|  Cindy|456.0|\n+----+-------+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill({\"Name\":\"No Name\",\"sales\":0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc686745-c32a-4ff9-83b5-c024f4f02e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69f57aab-5c4b-40ea-b80c-fc980becc596",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-------+-----+\n",
       "|  Id|   Name|Sales|\n",
       "+----+-------+-----+\n",
       "|emp1|   John|  0.0|\n",
       "|emp2|No Name|  0.0|\n",
       "|emp3|No Name|345.0|\n",
       "|emp4|  Cindy|456.0|\n",
       "+----+-------+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-------+-----+\n|  Id|   Name|Sales|\n+----+-------+-----+\n|emp1|   John|  0.0|\n|emp2|No Name|  0.0|\n|emp3|No Name|345.0|\n|emp4|  Cindy|456.0|\n+----+-------+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill({\"Name\":\"No Name\",\"sales\":0}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40f092cd-b551-4e98-8986-4b57828d0b79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A very common practice is to fill values with the mean value for the column, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c25d5c9-1db2-43d0-8c5f-6fef73114773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Out[20]: 400.5"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Out[20]: 400.5",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "\n",
    "# Weird nested formatting of Row object!\n",
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c33e6545-048a-4ab4-bd3f-3b6eea9b9785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Out[21]: Row(avg(Sales)=400.5)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Out[21]: Row(avg(Sales)=400.5)",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0da0205-1dfd-4231-ae06-1c96a48f684f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mean_sales = mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4211b714-aa54-434a-8a61-24506a99bab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John|400.5|\n",
       "|emp2| null|400.5|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John|400.5|\n|emp2| null|400.5|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill(mean_sales,[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d7328f4-c213-4fde-a3c3-69a32fe81fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John|400.5|\n",
       "|emp2| null|400.5|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John|400.5|\n|emp2| null|400.5|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.na.fill({\"Sales\":mean_sales}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5512234c-c55a-4121-a1ba-38485d4bc741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----+-----+-----+\n",
       "|  Id| Name|Sales|\n",
       "+----+-----+-----+\n",
       "|emp1| John|400.5|\n",
       "|emp2| null|400.5|\n",
       "|emp3| null|345.0|\n",
       "|emp4|Cindy|456.0|\n",
       "+----+-----+-----+\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+----+-----+-----+\n|  Id| Name|Sales|\n+----+-----+-----+\n|emp1| John|400.5|\n|emp2| null|400.5|\n|emp3| null|345.0|\n|emp4|Cindy|456.0|\n+----+-----+-----+\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One (very ugly) one-liner\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4a7fd6c-a19a-4969-a4c7-a81abb68fdcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That is all we need to know for now!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Missing_Data",
   "notebookOrigID": 1022319622863320,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
